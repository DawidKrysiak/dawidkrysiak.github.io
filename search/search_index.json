{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Obsidian Notes","text":"<p>Publish your public notes with MkDocs</p>"},{"location":"#hello-world","title":"Hello World!","text":"<p>The <code>index.md</code> in the <code>/docs</code> folder is the homepage you see here.</p> <p>The folders in <code>/docs</code> appear as the main sections on the navigation bar.</p> <p>The notes appear as pages within these sections. For example, Note 1 in <code>Topic 1</code></p>"},{"location":"AWS/AWS-CloudFront-function-to-redirect-empty-document-requests-to-index.html/","title":"CloudFront function to redirect empty document requests to index.html","text":""},{"location":"AWS/AWS-CloudFront-function-to-redirect-empty-document-requests-to-index.html/#url-rewrite-to-append-indexhtml-to-the-uri-for-single-page-applications","title":"URL rewrite to append index.html to the URI for single page applications","text":"<p>If you set up an s3 website serving content generated by Hugo or Jekyll, everything works fine, until you decide to use CloudFront to add SSL for example. The website breaks (Access Denied errors) as soon as you try to open the posts. The reason being that when you open <code>https://website.com/blog</code>, you are actually loading <code>https://website.com/blog/index.html</code>. You don't have to specify that document as the index.html is just default assumption that any web server should be dealing with automatically. Problem is, it doesn't work with CloudFront (because it is not a web server, neither is s3 and CF talks to the s3 bucket through REST API calls, not HTTP/S). Desperate users open their s3 buckets to the public, set up s3 website endpoint and accellerate that through CloudFront. But there is a better option. Amazon suggests to use a new shiny thing (as usual) - CloudFront functions and specifically url-rewrite-function. </p> <p>But wait, I can hear you say 'but there is the default root object setting in CF!' - yes, there is, but it only applies to the root document (so https://website.com/index.html not website.com/.../index.html) default root object</p>"},{"location":"AWS/AWS-CloudFront-function-to-redirect-empty-document-requests-to-index.html/#deployment-steps","title":"Deployment steps","text":""},{"location":"AWS/AWS-CloudFront-function-to-redirect-empty-document-requests-to-index.html/#clone-the-official-aws-repo","title":"Clone the official AWS repo","text":"<pre><code>git clone https://github.com/aws-samples/amazon-cloudfront-functions.git\n</code></pre>"},{"location":"AWS/AWS-CloudFront-function-to-redirect-empty-document-requests-to-index.html/#create-function","title":"Create function","text":"<pre><code>cd amazon-cloudfront-functions\n\naws cloudfront create-function --name url-rewrite-single-page-apps --function-config Comment=\"Function to redirect empty doc requests to index.html\",Runtime=cloudfront-js-1.0 --function-code fileb://url-rewrite-single-page-apps/index.js\n</code></pre> <p>If the function was created correctly, the JSON output should look similar to this (make note of the ETag value, you'll need it in a second!):</p> <pre><code>{\n    \"Location\": \"https://cloudfront.amazonaws.com/2020-05-31/function/arn:aws:cloudfront::&lt;account id&gt;:function/url-rewrite-single-page-apps\",\n    \"ETag\": \"EXXXXXXXXXXXX\",\n    \"FunctionSummary\": {\n        \"Name\": \"url-rewrite-single-page-apps\",\n        \"Status\": \"UNPUBLISHED\",\n        \"FunctionConfig\": {\n            \"Comment\": \"Function to redirect empty doc requests to index.html\",\n            \"Runtime\": \"cloudfront-js-1.0\"\n        },\n        \"FunctionMetadata\": {\n            \"FunctionARN\": \"arn:aws:cloudfront::&lt;account id&gt;:function/url-rewrite-single-page-apps\",\n            \"Stage\": \"DEVELOPMENT\",\n            \"CreatedTime\": \"2021-12-26T08:43:50.950Z\",\n            \"LastModifiedTime\": \"2021-12-26T08:43:50.950Z\"\n        }                     \n    }                    \n}\n</code></pre>"},{"location":"AWS/AWS-CloudFront-function-to-redirect-empty-document-requests-to-index.html/#test-the-function","title":"Test the function","text":"<p>To validate that the function is working as expected, you can use the JSON test objects in the <code>test-objects</code> directory. To test, use the <code>test-function</code> CLI command as shown in the following example:</p> <pre><code>$ aws cloudfront test-function --if-match &lt;ETag&gt; --name url-rewrite-single-page-apps --event-object fileb://url-rewrite-single-page-apps/test-objects/file-name-no-extension.json\n</code></pre> <p>If the function has been set up correctly, you should see the <code>uri</code> being updated to <code>index.html</code> in the <code>FunctionOutput</code> JSON object: <pre><code>{\n    \"TestResult\": {\n        \"FunctionSummary\": {\n            \"Name\": \"url-rewrite-single-page-apps\",\n            \"Status\": \"UNPUBLISHED\",\n            \"FunctionConfig\": {\n                \"Comment\": \"\",\n                \"Runtime\": \"cloudfront-js-1.0\"\n            },\n            \"FunctionMetadata\": {\n                \"FunctionARN\": \"arn:aws:cloudfront::1234567890:function/url-rewrite-single-page-apps\",\n                \"Stage\": \"DEVELOPMENT\",\n                \"CreatedTime\": \"2021-04-09T21:53:20.882000+00:00\",\n                \"LastModifiedTime\": \"2021-04-09T21:53:21.001000+00:00\"\n            }\n        },\n        \"ComputeUtilization\": \"14\",\n        \"FunctionExecutionLogs\": [],\n        \"FunctionErrorMessage\": \"\",\n        \"FunctionOutput\": \"{\\\"request\\\":{\\\"headers\\\":{\\\"host\\\":{\\\"value\\\":\\\"www.example.com\\\"},\\\"accept\\\":{\\\"value\\\":\\\"text/html\\\"}},\\\"method\\\":\\\"GET\\\",\\\"querystring\\\":{\\\"test\\\":{\\\"value\\\":\\\"true\\\"},\\\"arg\\\":{\\\"value\\\":\\\"val1\\\"}},\\\"uri\\\":\\\"/blog/index.html\\\",\\\"cookies\\\":{\\\"loggedIn\\\":{\\\"value\\\":\\\"false\\\"},\\\"id\\\":{\\\"value\\\":\\\"CookeIdValue\\\"}}}}\"\n    }\n}\n</code></pre></p>"},{"location":"AWS/AWS-CloudFront-function-to-redirect-empty-document-requests-to-index.html/#publish-the-function","title":"Publish the function.","text":"<p>Please note that the JSON response states <code>\"Status\" : \"UNPUBLISHED\"</code>, so the next step is to publish the function. <pre><code>aws cloudfront publish-function --name url-rewrite-single-page-apps --if-match &lt;ETag&gt;\n</code></pre> And - if successful - JSON response should look similar to:</p> <pre><code>{\n    \"FunctionSummary\": {\n        \"Name\": \"url-rewrite-single-page-apps\",\n        \"Status\": \"UNASSOCIATED\",\n        \"FunctionConfig\": {\n            \"Comment\": \"Function to redirect empty doc requests to index.html\",\n            \"Runtime\": \"cloudfront-js-1.0\"\n        },\n        \"FunctionMetadata\": {\n            \"FunctionARN\": \"arn:aws:cloudfront::&lt;account id&gt;:function/url-rewrite-single-page-apps\",\n            \"Stage\": \"LIVE\",\n            \"CreatedTime\": \"2021-12-26T08:47:42.111Z\",\n            \"LastModifiedTime\": \"2021-12-26T08:47:42.111Z\"\n        }\n    }\n}\n</code></pre>"},{"location":"AWS/AWS-CloudFront-function-to-redirect-empty-document-requests-to-index.html/#configuration-of-the-function","title":"Configuration of the function.","text":"<p>Since it's created and published, now it needs to be configured.</p> <p><pre><code>aws cloudfront get-distribution-config --id &lt;distribution name, NOT ETag!&gt; --output json &gt; dist-cfg.json\n</code></pre> Edit the <code>dist-cfg.json</code>:</p> <ul> <li>Change the <code>ETag</code> key to <code>IfMatch</code></li> <li>Modify <code>FunctionAssociation</code> to the following:</li> </ul> <pre><code>\"FunctionAssociations\": {\n                \"Quantity\": 1,\n                \"Items\" : [     \n                        {\n                        \"EventType\" : \"viewer-request\",\n                        \"FunctionARN\":\"arn:aws:cloudfront::&lt;account id&gt;:function/url-rewrite-single-page-apps\"\n                }\n                ]\n\n            },\n</code></pre>"},{"location":"AWS/AWS-CloudFront-function-to-redirect-empty-document-requests-to-index.html/#modify-cloudfront-distribution-by-adding-etag","title":"Modify CloudFront distribution by adding <code>ETag</code>.","text":"<p>Distributions-&gt;Distribution ID-&gt;Origins-&gt;Edit-&gt;Add custom header - optional Add <code>ETag</code>, value <code>EXXXXXXXXXXXX</code></p>"},{"location":"AWS/AWS-CloudFront-function-to-redirect-empty-document-requests-to-index.html/#update-the-distribution","title":"Update the distribution","text":"<pre><code>aws cloudfront update-distribution --id  &lt;CF Distribution ID&gt; --cli-input-json fileb://dist-cfg.json\n</code></pre>"},{"location":"AWS/AWS-CloudFront-function-to-redirect-empty-document-requests-to-index.html/#other-considerations","title":"Other considerations.","text":"<p>Somewhere along the way you should have locked down your s3 bucket so it's not public, and add a policy (can be done automatically through CloudFront-&gt;Distributions-&gt;-&gt;Edit Origin-&gt; s3 bucket access -&gt; Yes,use OAI (-&gt;create a new OAI)-&gt; Yes,update the bucket polcy) to allow for CloudFront access, so the Bucket Policy should look something like: <p><pre><code>{\n    \"Version\": \"2008-10-17\",\n    \"Id\": \"PolicyForCloudFrontPrivateContent\",\n    \"Statement\": [\n        {\n            \"Sid\": \"1\",\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"arn:aws:iam::cloudfront:user/CloudFront Origin Access Identity &lt;ID&gt;\"\n            },\n            \"Action\": \"s3:GetObject\",\n            \"Resource\": \"arn:aws:s3:::&lt;bucket name&gt;/*\"\n        }\n    ]\n}\n</code></pre> (c) Dawid Krysiak https://itisoktoask.me/ </p>"},{"location":"AWS/AWS-CloudWatch-Insights-count-visits-from-IP-addresses/","title":"CloudWatch Insights to find an abuser","text":"<p>Recently I had a case where a customer tried to figure out who is abusing their systems, probably by invoking bots. Fortunately, their application was recording registrations in the main application logs, so the solution was quite simple:</p> <pre><code>fields @message\n| filter @message like /POST \\\"\\/registration/\n| parse @message /(?&lt;@ip&gt;([0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3})[.]*)/\n| stats count() as requestCount by @ip\n| sort requestCount desc\n| filter requestCount &gt;5\n</code></pre>","tags":["AWS","EC2","CloudWatch","security"]},{"location":"AWS/AWS-CloudWatch-Insights-count-visits-from-IP-addresses/#lets-break-it-down","title":"let's break it down","text":"<ul> <li>first line selects the fields (in this case, just the body of the message</li> <li>second line filters out the lines that are relevant in this case - containing <code>POST \"/registration</code> string</li> <li>third line filters the remaining lines by looking for an regex matching the IP address format</li> <li>fourth line is the equivalent of 'count()'</li> <li>fifth line orders the results in descending order</li> <li>sixth line filters out only IPs that occurred more than 5 times</li> <li></li> </ul>","tags":["AWS","EC2","CloudWatch","security"]},{"location":"AWS/AWS-CloudWatch-Insights-to-analyse-apache-nginx-api-calls/","title":"AWS CloudWatch Insights to analyse apache nginx api calls","text":"<p>count the unique IP addresses in the apache/nginx/flow logs</p> <pre><code>fields @timestamp, @message\n  | parse @message /(?&lt;@ip&gt;([0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3})[.]*)/\n  | stats count() as requests by @ip\n  | sort requests desc\n</code></pre>"},{"location":"AWS/AWS-CloudWatch-Insights-to-analyse-apache-nginx-api-calls/#cloudwatch-insights-to-find-most-commonly-used-api-calls","title":"CloudWatch Insights to find most commonly used API calls","text":"<p>Recently I had a case where a customer tried to figure out what api calls were used in their system. Fortunately, 3rd-party application's logs contained references to the APIs in square brackets like</p> <pre><code>2023-05-10 14:27:53,794 INFO  168372887373144 connector:30 -  Start : [loginPage]\n</code></pre> <pre><code>fields @message, @timestamp\n| filter @message like /(Start)/\n| parse \"[*]\" as matchedValue\n| stats count() as requestCount by matchedValue\n| sort requestCount desc\n</code></pre> <p>result: loginPage: 100000</p> <p>(c) Dawid Krysiak https://itisoktoask.me/</p>"},{"location":"AWS/AWSConfig-generating-cost-while-disabled/","title":"AWS Config generating cost while 'disabled'","text":""},{"location":"AWS/AWSConfig-generating-cost-while-disabled/#aws-is-like-a-drug-dealer","title":"AWS is like a drug dealer","text":"<p>They will lure you in by free tier, never warn you about hidden cost and as they say - 'easy to get in, hard to get out'. They literally advertise '1-click setup'. But of course - there is no '1-click to delete' button :)</p>"},{"location":"AWS/AWSConfig-generating-cost-while-disabled/#the-hidden-cost-of-aws-config","title":"The hidden cost of AWS Config","text":"<p>Reading the official documentation of AWS Config one could think that all they need to do to stop incurring cost is simply disable the collector/recorder. So that's what they do, and they walk away. And it takes a while to notice a small charge in the billing dashboard regarding 'config'.</p>"},{"location":"AWS/AWSConfig-generating-cost-while-disabled/#what-they-dont-tell-you","title":"What they don't tell you","text":"<p>Even if the recorder is disabled, it still incurs cost - in my case it was $0.24/mo, so negligible (but still annoying). And removing that recorder through the interface or disabling AWS Config through the interface is not possible ('easy to get in', remember?)</p>"},{"location":"AWS/AWSConfig-generating-cost-while-disabled/#solution-1-if-you-actually-use-aws-config","title":"Solution 1: if you actually use AWS Config","text":"<p>Enable retention policy. By default, AWS config retains the collection results indefinitely. By introducing lifecycle policy, you can delete collections older than x number of days.</p>"},{"location":"AWS/AWSConfig-generating-cost-while-disabled/#solution-2-if-you-dont-need-aws-config","title":"Solution 2: if you don't need AWS Config","text":"<p>You need an IAM user with sufficient permissions to modify <code>configservice</code>, configure AWS CLI in your system so you can make an API call.</p>"},{"location":"AWS/AWSConfig-generating-cost-while-disabled/#list-available-recorders","title":"List available recorders","text":"<pre><code>aws configservice describe-configuration-recorders\n</code></pre>"},{"location":"AWS/AWSConfig-generating-cost-while-disabled/#delete-recorder","title":"Delete recorder","text":"<p><pre><code>aws configservice delete-configuration-recorder --configuration-recorder-name &lt;name - usually `default`&gt;\n</code></pre> Just to make sure it doesn't come back, remove the AWSConfig role from IAM Roles.</p> <p>(c) Dawid Krysiak https://itisoktoask.me/ </p>"},{"location":"AWS/Amazon-Linux-2022-2023-tempfs-configuration/","title":"Amazon Linux 2022, 2023 tempfs configuration","text":""},{"location":"AWS/Amazon-Linux-2022-2023-tempfs-configuration/#things-to-know-when-migrating-from-amazon-linux-1-or-amazon-linux-2-to-amazon-linux-2022-tempfs","title":"Things to know when migrating from Amazon Linux 1 or Amazon Linux 2 to Amazon Linux 2022 - tempfs","text":"<p>Amazon Linux 2022 comes equipped with /tmp directory mounted in memory. It's visible in <code>df</code> as a separate device</p> <pre><code>/dev/xvda1      8.0G  4.2G  3.9G  52% /\ntmpfs           986M     0  986M   0% /tmp\n</code></pre> <p>Rather than a fixed entry in /etc/fstab, it uses a service to remount the /tmp after a reboot</p> <pre><code>/usr/lib/systemd/system/tmp.mount\n</code></pre>"},{"location":"AWS/Amazon-Linux-2022-2023-tempfs-configuration/#benefits","title":"Benefits","text":"<p>Tempfs is fast. Absurdly fast for something that looks like 'a storage'. The service definition mentioned above automatically allocates 50% of RAM as possible storage.</p>"},{"location":"AWS/Amazon-Linux-2022-2023-tempfs-configuration/#drawbacks","title":"Drawbacks","text":"<p>If your software depends on a large volume of temporary files, you have to carefully measure the performance -  if you fill in the /tmp with your files, and the demand for RAM from application raises, system will start using swap and that will obviously have a massive impact on performance</p>"},{"location":"AWS/Amazon-Linux-2022-2023-tempfs-configuration/#how-to-disable-it","title":"How to disable it","text":"<p>As mentioned earlier, it's mounted after each reboot via a service, so it is as simple as stopping the service and then disabling it.</p> <p><pre><code>sudo systemctl disable tmp.mount\nsudo systemctl stop tmp.mount\n</code></pre> But now the /tmp is useless, so let's recreate it.</p> <pre><code>sudo rm -rf /tmp\nsudo mkdir /tmp\nsudo chmod 1777 /tmp\n</code></pre> <p>test:  <code>df -h</code> should show you no /tmp as a separate mountpoint.</p>"},{"location":"AWS/Amazon-Linux-2023-lessons-learned-so-far/","title":"Things to know when migrating from Amazon Linux 1 or Amazon Linux 2 to Amazon Linux 2023","text":"<p>AmazonLinux 2023 is based on Fedora Linux which might be a good thing or a bad thing (let's not go there :) ), which gives a lot of head start for many sysadmins. I am positively surprised with this edition (which builds up on the experience of AL2022)  - the system is completely debloated, with no pre-installed languages, libraries, services etc... That approach has two outcomes: * Positive: the system is noticeably 'faster' (more precisely: more responsive) and you don't pay for running stuff (both storage and compute) you'll never use (or in the worst case scenario will collide with your software) * Negative: DIY all the way. Do you want something running? Figure out what you need, find the packages, install them, and configure the system the way YOU want it.</p>"},{"location":"AWS/Amazon-Linux-2023-lessons-learned-so-far/#tmp-handling","title":"/tmp handling","text":"<p>Amazon Linux 2023 comes equipped with /tmp directory mounted in memory. It's visible in <code>df</code> as a separate device</p> <pre><code>/dev/xvda1      8.0G  4.2G  3.9G  52% /\ntmpfs           986M     0  986M   0% /tmp\n</code></pre> <p>Rather than a fixed entry in /etc/fstab, it uses a service to remount the /tmp after a reboot</p> <pre><code>/usr/lib/systemd/system/tmp.mount\n</code></pre>"},{"location":"AWS/Amazon-Linux-2023-lessons-learned-so-far/#benefits","title":"Benefits","text":"<p>Tempfs is fast. Absurdly fast for something that looks like 'a storage'. The service definition mentioned above automatically allocates 50% of RAM as possible storage.</p>"},{"location":"AWS/Amazon-Linux-2023-lessons-learned-so-far/#drawbacks","title":"Drawbacks","text":"<p>If your software depends on a large volume of temporary files (you know who you are :)), you have to carefully measure the performance -  if you fill in the /tmp with your files, and the demand for RAM from the application rises, the system will start using swap and that will have a massive impact on performance</p>"},{"location":"AWS/Amazon-Linux-2023-lessons-learned-so-far/#how-to-disable-it","title":"How to disable it","text":"<p>As mentioned earlier, it's mounted after each reboot via a service, so it is as simple as stopping the service and then disabling it.</p> <p><pre><code>sudo systemctl disable tmp.mount\nsudo systemctl stop tmp.mount\n</code></pre> But now the /tmp is useless, so let's recreate it.</p> <pre><code>sudo rm -rf /tmp\nsudo mkdir /tmp\nsudo chmod 1777 /tmp\n</code></pre> <p>test:  <code>df -h</code> should show you no /tmp as a separate mount point.</p>"},{"location":"AWS/Amazon-Linux-2023-lessons-learned-so-far/#system-logging","title":"System Logging","text":"<p>Sysadmins got used to the fact that their OSes come preinstalled with some sort of system-level logging. Some applications just automatically assume you have syslog or similar up and running and try to dump their output to /var/log/messages. AmazonLinux 2023 comes with an unprecedented level of respect to the sysadmins and does not come preconfigured (well..there is journald as part of SystemD, but that's not what we are talking about here). So... When you set up your system, remember to include syslog, rsyslog etc... into your deployment process</p>"},{"location":"AWS/Amazon-Linux-2023-lessons-learned-so-far/#python","title":"Python","text":"<p>Python is a dependency on many system-level tools (e.g. yum) so it needs to come preinstalled for the tools to work. But... If you type <code>python</code> into your console, no results will be found - python is not added to your profile's paths. So you have two choices - add python to the path (through <code>export PATH=$PATH;...</code>) or - as per new recommendations -set up a virtual environment (https://docs.python.org/3/library/venv.html) or even set up docker containers with the required versions of Python for your code -that way you won't accidentally mess up the system-level settings, breaking your system tools in the process. Separation of workflows is always a good idea in my book.</p>"},{"location":"AWS/Amazon-Linux-2023-lessons-learned-so-far/#systemv-legacy-support","title":"SystemV / legacy support","text":"<p>You can still set up services in sysvinit (/etc/init.d/) but the system will let you know it's not ecstatic about it and it will be quite verbose.</p> <pre><code>SysV service '/etc/rc.d/init.d/cfn-hup' lacks a native systemd unit file. Automatically generating a unit file for compatibility. Pleaseupdate package to include a native systemd unit file, in order to make it more safe and robust.\n</code></pre>"},{"location":"AWS/Metabase-on%20EC2-Amazon-Linux-2-tutorial/","title":"Metabase on Amazon Linux 2","text":""},{"location":"AWS/Metabase-on%20EC2-Amazon-Linux-2-tutorial/#this-tutorial-covers-deployment-steps-of-metabase-system-using-amazon-linux-2","title":"This tutorial covers deployment steps of Metabase system using Amazon Linux 2.","text":""},{"location":"AWS/Metabase-on%20EC2-Amazon-Linux-2-tutorial/#preface","title":"Preface:","text":"<p>I started this project with 'best practices as understood by Amazon' in mind - decoupled, scalable, fault resistant... And then realized, that I know a few really heavy production systems that don't have any of that and... function very well. With dozens of users working simultaneously. So as much as it's fun to deploy complicated infrastructure, it only benefits Amazon. Metabase database is tiny (less than 10MB?) my source of information - a single table with tens of thousands of rows - 20MB (in reality you will query other database sources that exist for their own purpose so your needs will be even smaller!) - why would I create a separate RDS (or two!) for that? With snapshots? Multi-AZ? Why would I bother with ElasticBeanstalk, and cloudforming it? The ONLY benefit is that Jeff has more money to pay his alimony (and for his space trips). Need this to be portable/redeployable? Just make an AMI after initial deployment (before any data sources are added to metabase), share it with regions/accounts you wish, deploy there (remember to chage the passwords :) ) - happy days. Cheap and easy solution. Spend your money on keeping regular snapshots, AMIs (e.g. properly set up AWS Backup!) and coffee, not on infastructure that serves no purpose other than some imaginary brownie points. If you insist of having a fancy infra, follow Metabase's own tutorial Metabase on ElasticBeanstalk (which at the time of writing this tutorial, didn't actually worked for me).</p> <p>One thing that absolutely needs to happen is - the backbone database (which metabase uses to store user accounts, system settings, 'questions', graphs settings and so on) need to be moved away from the default file-based h2 format to a real database. H2 is not fit for any production purpose. And the developers of Metabase will agree with me - repeatedly, all over the place. DO NOT use the default H2 backbone for production. A crash (e.g. forceful kill of a process by <code>kill -9 pid</code> of the app (jar) while it's doing any updates to the tables WILL corrupt the data, and to my experience so far, NONE of the available tutorials work. EVER (yeah, ask me how I know :( ). Yes, H2 has 100% failure rate in my experience. The only solution is to start over or (if you are smart enough to have EBS snapshots every night :) ) to revert back the whole system.</p> <p>Having the above in mind, the project comprise of:</p> <ul> <li>a single EC2 instance</li> <li> MySQL server installed on the instance itself, not as RDS. But if you insist - RDS works just fine, MySQL/MariaDB and PostgreSQL, that's how I started, before I realized I'm burning cash. RDS in my specific case could be of benefit (as mentioned I have my own 'data source' that needs to be queried), but in usual situations - you will be querying other RDS systems ANYWAY.</li> <li>Metabase running as a SystemD service</li> </ul>"},{"location":"AWS/Metabase-on%20EC2-Amazon-Linux-2-tutorial/#instance","title":"Instance.","text":"<p>Deploy an instance in the desired region. Instance type depends on your needs. In my case, t2.micro was sufficient during development, but even then, machine was left with 60MB of available RAM, so 2GB of RAM is absolute minimum if you have more than one user working on it (it will work with 1GB RAM + a bit of swap, but it will be horribly slow in regenerating graphs). I would say that t2.small is a good compromise, t2.medium should be plenty, unless you have dozens of users working at the same time. If you wish to save money on the project even further, consider if the system has to work 24/7 - maybe you have fixed working hours beyond which you can set up the maintenance window for updates/patching and then shut down the system overnight? Consider using Lambda functions for start/stop GitHub</p> <p>While you are preparing the instance, you might just as well set up the SecurityGroup at the same time.</p> <ul> <li>22 (for ssh access)</li> <li>306 (if you wish to access MySQL remotely rather than working on the instance itself all the time. In this case, I suggest locking down the access to a single IP rather than leaving it for 0/0. Or better yet, **don't open 3306 to the world at all**, and use SSH tunnel for your connection to MySQL [SSH tunnel for MySQL](https://itisoktoask.me/linux-useful-oneliners/#mysql-tunnel). Less stable, but surely more secure).</li> <li>3000 (that's the default Metabase port. Some people add nginx to the mix, to allow for 80/443 access, but I can list a few reasons why I don't bother, just give users a link with :3000 in it and you are sorted :) RAM saved, work spared :)</li> </ul> <p>Storage: I left it at the default 8GB ssd, after the deployment was done, <code>du</code> was at 63% and I don't see it growing very much over time (I introduced logrotate just in case, anticipating the logs being the biggest contributors to storage consumption).</p> <p>Add swapfile.</p> <pre><code>sudo fallocate -l 1G /swapfile\nsudo chmod 600 /swapfile\nsudo mkswap /swapfile &amp;&amp; sudo swapon /swapfile\n</code></pre> <p>add to /etc/fstab to make swap mounted after reboot.</p> <pre><code>sudo echo '/swapfile swap swap defaults 0 0' &gt;&gt;/etc/fstab\n</code></pre>"},{"location":"AWS/Metabase-on%20EC2-Amazon-Linux-2-tutorial/#mysql","title":"MySQL","text":"<pre><code>sudo yum update -y\nsudo yum install mysql-community-server \nsudo systemctl enable mysqld\nsudo systemctl start mysqld\n</code></pre> <p>Now the first moment that I got a bit confused - I don't have to deploy MySQL these days (I rather deploy an RDS and handover to customers to play with), so 'I missed the memo'. Some of the key processes has changed, so making note of it here.</p> <p>You don't just connect to mysql as root without password and set it up using a 'combo' 'grant all privileges...'</p> <pre><code>sudo grep 'temporary password' /var/log/mysqld.log \n</code></pre> <p>Make a note of the temporary root password generated by the installer.</p> <p><pre><code>sudo mysql_secure_installation \n</code></pre> Use the password noted above, set a new one, remove anonymous users, decide if you require for root to be able to log in remotely (don't, use the tunnel instead, as mentioned earlier), reload permissions. Done.</p> <p>Connect to the MySQL console, create a new db - let's call it <code>backbone</code></p> <pre><code>CREATE DATABASE backbone;\nUSE backbone;\nCREATE USER 'metabase'@'localhost' IDENTIFIED BY '&lt;password&gt;';\nGRANT ALL ON backbone.* TO 'metabase'@'localhost';\n</code></pre>"},{"location":"AWS/Metabase-on%20EC2-Amazon-Linux-2-tutorial/#metabase-user-setup","title":"Metabase user setup","text":"<p>Your instance (given how tiny the specs are) should be solely dedicated to metabase purpose, so the below method is a total overkill, you could run everything from the default ec2-user, but I believe it's a good practice to separate out the services and data from a default system login, to avoid accidental damage.</p> <pre><code>sudo adduser metabase\nsudo touch /var/log/metabase.log\nsudo chown syslog:adm /var/log/metabase.log\nsudo touch /etc/default/metabase\n</code></pre> <p>I made sure in /etc/passwd that my metabase user can't login to the system as a human :) </p> <pre><code>metabase:x:995:993::/home/metabase:/bin/false \n</code></pre>"},{"location":"AWS/Metabase-on%20EC2-Amazon-Linux-2-tutorial/#java-installation","title":"Java installation","text":"<pre><code>sudo amazon-linux-extras install java-openjdk11\n</code></pre>"},{"location":"AWS/Metabase-on%20EC2-Amazon-Linux-2-tutorial/#metabase-download","title":"Metabase download","text":"<p>Admittedly I was working as root at this time, so make sure permissions are set right after.</p> <pre><code>cd /home/metabase/\nwget https://downloads.metabase.com/v0.40.1/metabase.jar\nchown metabase:metabase metabase.jar\n</code></pre> <p>Of course, in the future, make sure you download the latest version.</p>"},{"location":"AWS/Metabase-on%20EC2-Amazon-Linux-2-tutorial/#set-up-the-service","title":"Set up the service","text":"<pre><code>touch /etc/systemd/system/metabase.service\n</code></pre> <p>Use your favourite text editor to add the following. Make sure the users (system and MySQL),password are set to what you set up in your system. You can also change the StandardOutput, StandardError variables to separate the messages to another file rather than dump everything into /var/log/messages I'm doing it this way, to make my life simpler. a. this instance's sole purpose is to run metabase + mysql, so I don't have to monitor multiple streams of possible errors b. when I set up the CloudWatch agent, I have a template to push out system logs already, don't have to fiddle with it to add metabase specific logs. One less thing to remember :). But of course - you do what suits your needs. </p> <pre><code>[Unit]\nDescription=Metabase server\n\n[Service]\nType=simple\nWorkingDirectory=/home/metabase\nUser=metabase\nGroup=metabase\nExecStart=/bin/java -jar /home/metabase/metabase.jar\nRestart=on-failure\nRestartSec=10\nStandardOutput=syslog\nStandardError=syslog\nSyslogIdentifier=metabase\nEnvironment=MB_PASSWORD_COMPLEXITY=normal\nEnvironment=MB_PASSWORD_LENGTH=8\nEnvironment=MB_DB_TYPE=mysql\nEnvironment=MB_DB_DBNAME=backbone\nEnvironment=MB_DB_PORT=3306\nEnvironment=MB_DB_USER=&lt;user&gt;\nEnvironment=MB_DB_PASS=&lt;password&gt;\nEnvironment=MB_DB_HOST=&lt;host&gt;\nEnvironmen=MB_EMOJI_IN_LOGS=true\n</code></pre> <p>Moment of truth :). Start the service.</p> <pre><code>systemctl enable  metabase\nsystemctl start metabase\n</code></pre> <p>Test.</p> <p><pre><code>ps aux | grep metabase\n</code></pre> You should see the process running. If you don't, check <code>/var/log/messages</code> - metabase should dump errors there. In my case it was MySQL permissions issues. If it's running, we are almost there, just one important test - check if metabase is using MySQL rather than h2. </p> <pre><code>sudo ls -lha /home/metabase\n</code></pre> <p>The only elements you should see is metabase.jar and plugins directory. If you see any <code>.db.</code> files, it means that the connection to MySQL was not picked up from the config and metabase defaulted to its built-in functionality. DO NOT LEAVE IT AS IS. Investigate, fix, remove the db files, try again. Don't 'let it be, should be fine' - you will regret later.</p>"},{"location":"AWS/Metabase-on%20EC2-Amazon-Linux-2-tutorial/#testing","title":"Testing","text":"<p>It's time to log in to metabase - using a browser, go to  http://:3000/ you should see the Metabase interface, go through the setup process, add your sources and have fun."},{"location":"AWS/Metabase-on%20EC2-Amazon-Linux-2-tutorial/#updates","title":"Updates","text":"<p>Stop the service</p> <pre><code>systemctl stop metabase\n</code></pre> <p>Back up the current version</p> <pre><code>cd /home/metabase\nmv metabase.jar metabase_$(date +%d-%m-%Y).jar\n</code></pre> <p>Download the latest version to the same place</p> <pre><code>wget https://downloads.metabase.com/{latest-version}/metabase.jar\n</code></pre> <p>Start the service</p> <pre><code>systemctl start metabase\n</code></pre>"},{"location":"AWS/Metabase-on%20EC2-Amazon-Linux-2-tutorial/#there-are-some-useful-tips-that-got-me-through-my-stumbles","title":"There are some useful tips that got me through my stumbles.","text":"<p>MySQL developers in their 'wisdom' decided to introduce certain settings that caused a lot of frustration. One of them was inability to do 'group by' in some scenarios.  I know I could rework the metabase 'questions', but life is too short for that.</p> <p>First, check the sql_mode by running this query:</p> <pre><code>SELECT @@sql_mode;\n</code></pre> <p>The result should be something like this:</p> <pre><code>ONLY_FULL_GROUP_BY,STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_ENGINE_SUBSTITUTION\n</code></pre> <p>Make a note of the values, you will be needing most of them. Edit my.cnf under [mysqld] section:</p> <p>sql_mode= (values from above, but remove ONLY_FULL_GROUP_BY)</p> <p>e.g. </p> <pre><code>sql_mode=STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_ENGINE_SUBSTITUTION\n</code></pre> <pre><code>sudo systemctl restart mysqld.service\n</code></pre> <p>(c) Dawid Krysiak https://itisoktoask.me/ </p>"},{"location":"AWS/Metabase-on-EC2-Amazon-Linux-2022-2023/","title":"Metabase on EC2 Amazon Linux 2022 - tutorial","text":""},{"location":"AWS/Metabase-on-EC2-Amazon-Linux-2022-2023/#this-tutorial-covers-deployment-steps-of-metabase-system-using-amazon-linux-20222023","title":"This tutorial covers deployment steps of Metabase system using Amazon Linux 2022,2023.","text":""},{"location":"AWS/Metabase-on-EC2-Amazon-Linux-2022-2023/#preface","title":"Preface:","text":"<p>It's a follow-up to the previous tutorial based on Amazon Linux 2 (now deprecated as AL2 has very little time left, so let's all migrate to AL2023, shall we?).</p> <p>Amazon Linux 2023 officially admits its roots in Fedora 38. From a 'console user'-perspective, that doesn't change much (apart from an inclination to use <code>dnf</code> rather than <code>yum</code> for package management). There are some interesting configuration choices as well - for example /tmp is mounted in RAM-disk which makes it absurdly fast but comes at a penalty of limited space. So if your software rely on thousands of files in /tmp (you know who you are :)) then you might want to consider remounting /tmp and disabling the service that keeps mounting it. More on that in another article soon.</p> <p>Now to the fun part - deploy Metabase on AmazonLinux 2022. The below is 90% copy of the previous article with appropriate changes for the new release. Enjoy.</p> <p>I started this project with 'best practices as understood by Amazon' in mind - decoupled, scalable, fault resistant... And then realized, that I know a few really heavy production systems that don't have any of that and... function very well. With dozens of users working simultaneously. So as much as it's fun to deploy complicated infrastructure, it only benefits Amazon. Metabase database is tiny (less than 10MB?) my source of information - a single table with tens of thousands of rows - 20MB (in reality you will query other database sources that exist for their own purpose so your needs will be even smaller!) - why would I create a separate RDS (or two!) for that? With snapshots? Multi-AZ? Why would I bother with ElasticBeanstalk, and cloudforming it? The ONLY benefit is that Jeff has more money to pay his alimony (and for his space trips). Need this to be portable/redeployable? Just make an AMI after initial deployment (before any data sources are added to metabase), share it with regions/accounts you wish, deploy there (remember to chage the passwords :) ) - happy days. Cheap and easy solution. Spend your money on keeping regular snapshots, AMIs (e.g. properly set up AWS Backup!) and coffee, not on infastructure that serves no purpose other than some imaginary brownie points. If you insist of having a fancy infra, follow Metabase's own tutorial Metabase on ElasticBeanstalk (which at the time of writing this tutorial, didn't actually worked for me).</p> <p>One thing that absolutely needs to happen is - the backbone database (which metabase uses to store user accounts, system settings, 'questions', graphs settings and so on) need to be moved away from the default file-based h2 format to a real database. H2 is not fit for any production purpose. And the developers of Metabase will agree with me - repeatedly, all over the place. DO NOT use the default H2 backbone for production. A crash (e.g. forceful kill of a process by <code>kill -9 pid</code> of the app (jar) while it's doing any updates to the tables WILL corrupt the data, and to my experience so far, NONE of the available tutorials work. EVER (yeah, ask me how I know :( ). Yes, H2 has 100% failure rate in my experience. The only solution is to start over or (if you are smart enough to have EBS snapshots every night :) ) to revert back the whole system.</p> <p>Having the above in mind, the project comprise of:</p> <ul> <li>a single EC2 instance</li> <li> MySQL server installed on the instance itself, not as RDS. But if you insist - RDS works just fine, MySQL/MariaDB and PostgreSQL, that's how I started, before I realized I'm burning cash. RDS in my specific case could be of benefit (as mentioned I have my own 'data source' that needs to be queried), but in usual situations - you will be querying other RDS systems ANYWAY.</li> <li>Metabase running as a SystemD service</li> </ul>"},{"location":"AWS/Metabase-on-EC2-Amazon-Linux-2022-2023/#instance","title":"Instance.","text":"<p>Amazon Linux 2022 preview is available only in Oregon, so if you wish to deploy it in any other region, you have to copy the AMI. Instance type depends on your needs. In my case, t2.micro was sufficient during development, but even then, machine was left with 60MB of available RAM, so 2GB of RAM is absolute minimum if you have more than one user working on it (it will work with 1GB RAM + a bit of swap, but it will be horribly slow in regenerating graphs). I would say that t2.small is a good compromise, t2.medium should be plenty, unless you have dozens of users working at the same time. If you wish to save money on the project even further, consider if the system has to work 24/7 - maybe you have fixed working hours beyond which you can set up the maintenance window for updates/patching and then shut down the system overnight? Consider using Lambda functions for start/stop GitHub</p> <p>While you are preparing the instance, you might just as well set up the SecurityGroup at the same time.</p> <ul> <li>22 (for ssh access)</li> <li>3306 (if you wish to access MySQL remotely rather than working on the instance itself all the time. In this case, I suggest locking down the access to a single IP rather than leaving it for 0/0. Or better yet, don't open 3306 to the world at all, and use SSH tunnel for your connection to MySQL [SSH tunnel for MySQL](https://itisoktoask.me/linux-useful-oneliners/#mysql-tunnel). Less stable, but surely more secure).  </li> <li>3000 (that's the default Metabase port. Some people add nginx to the mix, to allow for 80/443 access, but I can list a few reasons why I don't bother, just give users a link with :3000 in it and you are sorted :) RAM saved, work spared :) of course, if you need a domain and SSL certificate, you either put more work into it, or engage ELB, it all depends on your needs</li> </ul>"},{"location":"AWS/Metabase-on-EC2-Amazon-Linux-2022-2023/#add-iam-role-to-the-instance","title":"Add IAM Role to the instance","text":"<p>I suggest adding the following AWS-managed policies which will future-proof your instance if you start thinking about properly managing it and take it out of a sandbox: <pre><code>arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore\narn:aws:iam::aws:policy/AmazonSSMPatchAssociation\n</code></pre></p>"},{"location":"AWS/Metabase-on-EC2-Amazon-Linux-2022-2023/#add-ssm-access-because-ssh-is-so-2003","title":"Add SSM access, because SSH is so 2003 :)","text":"<pre><code>sudo yum install -y https://s3.amazonaws.com/ec2-downloads-windows/SSMAgent/latest/linux_amd64/amazon-ssm-agent.rpm\n</code></pre>"},{"location":"AWS/Metabase-on-EC2-Amazon-Linux-2022-2023/#storage","title":"Storage:","text":"<p>I left it at the default 8GB ssd, after the deployment was done, <code>du</code> was at 63% and I don't see it growing very much over time (I introduced logrotate just in case, anticipating the logs being the biggest contributors to storage consumption).</p>"},{"location":"AWS/Metabase-on-EC2-Amazon-Linux-2022-2023/#add-swapfile","title":"Add swapfile.","text":"<pre><code>sudo fallocate -l 1G /swapfile\nsudo chmod 600 /swapfile\nsudo mkswap /swapfile &amp;&amp; sudo swapon /swapfile\n</code></pre>"},{"location":"AWS/Metabase-on-EC2-Amazon-Linux-2022-2023/#add-to-etcfstab-to-make-swap-mounted-after-reboot","title":"add to /etc/fstab to make swap mounted after reboot.","text":"<pre><code>sudo echo '/swapfile swap swap defaults 0 0' &gt;&gt;/etc/fstab\n</code></pre>"},{"location":"AWS/Metabase-on-EC2-Amazon-Linux-2022-2023/#mysql","title":"MySQL","text":"<p>That was a very frustrating stage as AL2022 is not an official distro yet, so default YUM/DNF configurations are useless. Once the AMI is released, I'm sure AWS will arm it with all sorts of repo definitions, but for now, here is my hacky way of deploying it.</p> <p><pre><code>yum install -y  https://dev.mysql.com/get/mysql80-community-release-fc35-3.noarch.rpm\n</code></pre> Ok, let's do yum update - FAIL - as mentioned above - distro doesn't exist and it's not advertising it as FedoraCore distro, so let's hack  the repo file to let it think it's FedoraCore35 :)</p> <p>edit the below with your editor of choice: <pre><code>/etc/yum.repos.d/mysql-community.repo\n</code></pre> add the following content (or manually change the existing one to match)</p> <pre><code>[mysql80-community]\nname=MySQL 8.0 Community Server\nbaseurl=http://repo.mysql.com/yum/mysql-8.0-community/fc/35/x86_64\nenabled=1\ngpgcheck=1\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-mysql-2022\n       file:///etc/pki/rpm-gpg/RPM-GPG-KEY-mysql\n\n[mysql-connectors-community]\nname=MySQL Connectors Community\nbaseurl=http://repo.mysql.com/yum/mysql-connectors-community/fc/35/x86_64\nenabled=1\ngpgcheck=1\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-mysql-2022\n       file:///etc/pki/rpm-gpg/RPM-GPG-KEY-mysql\n\n[mysql-tools-community]\nname=MySQL Tools Community\nbaseurl=http://repo.mysql.com/yum/mysql-tools-community/fc/35/x86_64\nenabled=1\ngpgcheck=1\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-mysql-2022\n       file:///etc/pki/rpm-gpg/RPM-GPG-KEY-mysql\n\n[mysql-tools-preview]\nname=MySQL Tools Preview\nbaseurl=http://repo.mysql.com/yum/mysql-tools-preview/fc/35/x86_64\nenabled=0\ngpgcheck=1\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-mysql-2022\n       file:///etc/pki/rpm-gpg/RPM-GPG-KEY-mysql\n</code></pre> <p>Let's see if that worked</p> <pre><code>dnf update -y\nsudo dnf install -y mysql-community-server\n</code></pre> <p>FAIL. Library I'm not familiar with is missing. A bit of digging and found a package providing it.</p> <pre><code> yum install -y wget https://rpmfind.net/linux/fedora/linux/releases/35/Everything/x86_64/os/Packages/m/mecab-0.996-3.fc35.2.x86_64.rpm\n</code></pre> <p>OK, let's try again.</p> <p><pre><code>sudo dnf install -y mysql-community-server\n</code></pre> It should install now, but I received an interesting notification about release upgrade. </p> <p><pre><code>dnf update --releasever=2022.0.20220719\n</code></pre> Once that's done, let's check if MySQL is running - probably not. But is the service defined? <pre><code>systemctl| grep mysql\n</code></pre> <code>mysqld.service</code> should be on the list already so let's enable it and start.</p> <p><code>sudo systemctl enable mysqld.service</code> <code>43  sudo systemctl start mysqld.service</code></p> <p>Is it running?</p> <p><code>ps aux | grep mysql</code></p>"},{"location":"AWS/Metabase-on-EC2-Amazon-Linux-2022-2023/#run-mysql-initial-setup","title":"Run MySQL initial setup.","text":"<pre><code>sudo grep 'temporary password' /var/log/mysqld.log \n</code></pre> <p>Make a note of the temporary root password generated by the installer.</p> <p><pre><code>sudo mysql_secure_installation \n</code></pre> Use the password noted above, set a new one, remove anonymous users, decide if you require for root to be able to log in remotely (don't, use the tunnel instead, as mentioned earlier), reload permissions. Done.</p> <p>Connect to the MySQL console, create a new db - let's call it <code>backbone</code></p> <pre><code>CREATE DATABASE backbone;\nUSE backbone;\nCREATE USER 'metabase'@'localhost' IDENTIFIED BY '&lt;password&gt;';\nGRANT ALL ON backbone.* TO 'metabase'@'localhost';\n</code></pre>"},{"location":"AWS/Metabase-on-EC2-Amazon-Linux-2022-2023/#metabase-user-setup","title":"Metabase user setup","text":"<p>Your instance (given how tiny the specs are) should be solely dedicated to metabase purpose, so the below method is a total overkill, you could run everything from the default ec2-user, but I believe it's a good practice to separate out the services and data from a default system login, to avoid accidental damage.</p> <pre><code>sudo adduser metabase\nsudo touch /etc/default/metabase\n</code></pre> <p>I made sure in /etc/passwd that my metabase user can't login to the system as a human :) </p> <pre><code>metabase:x:995:993::/home/metabase:/bin/false \n</code></pre>"},{"location":"AWS/Metabase-on-EC2-Amazon-Linux-2022-2023/#java-installation","title":"Java installation","text":"<pre><code>sudo dnf install -y java-17-openjdk-devel\n</code></pre>"},{"location":"AWS/Metabase-on-EC2-Amazon-Linux-2022-2023/#metabase-download","title":"Metabase download","text":"<p>Admittedly I was working as root at this time, so make sure permissions are set right after.</p> <pre><code>cd /home/metabase/\nwget https://downloads.metabase.com/v0.40.1/metabase.jar\nchown metabase:metabase metabase.jar\n</code></pre> <p>Of course, in the future, make sure you download the latest version.</p>"},{"location":"AWS/Metabase-on-EC2-Amazon-Linux-2022-2023/#set-up-the-service","title":"Set up the service","text":"<pre><code>touch /etc/systemd/system/metabase.service\n</code></pre> <p>Use your favourite text editor to add the following. Make sure the users (system and MySQL),password are set to what you set up in your system. You can also change the StandardOutput, StandardError variables to separate the messages to another file rather than dump everything into /var/log/messages I'm doing it this way, to make my life simpler. a. this instance's sole purpose is to run metabase + mysql, so I don't have to monitor multiple streams of possible errors b. when I set up the CloudWatch agent, I have a template to push out system logs already, don't have to fiddle with it to add metabase specific logs. One less thing to remember :). But of course - you do what suits your needs. </p> <pre><code>[Unit]\nDescription=Metabase server\n\n[Service]\nType=simple\nWorkingDirectory=/home/metabase\nUser=metabase\nGroup=metabase\nExecStart=/bin/java -jar /home/metabase/metabase.jar\nRestart=on-failure\nRestartSec=10\nStandardOutput=syslog\nStandardError=syslog\nSyslogIdentifier=metabase\nEnvironment=MB_PASSWORD_COMPLEXITY=normal\nEnvironment=MB_PASSWORD_LENGTH=8\nEnvironment=MB_DB_TYPE=mysql\nEnvironment=MB_DB_DBNAME=backbone\nEnvironment=MB_DB_PORT=3306\nEnvironment=MB_DB_USER=&lt;user&gt;\nEnvironment=MB_DB_PASS=&lt;password&gt;\nEnvironment=MB_DB_HOST=localhost\nEnvironmen=MB_EMOJI_IN_LOGS=true\n</code></pre> <p>Moment of truth :). Start the service.</p> <pre><code>systemctl enable  metabase\nsystemctl start metabase\n</code></pre> <p>Test.</p> <p><pre><code>ps aux | grep metabase\n</code></pre> You should see the process running. If you don't, check <code>/var/log/messages</code> - metabase should dump errors there. In my case it was MySQL permissions issues. If it's running, we are almost there, just one important test - check if metabase is using MySQL rather than h2. </p> <pre><code>sudo ls -lha /home/metabase\n</code></pre> <p>The only elements you should see is metabase.jar and plugins directory. If you see any <code>.db.</code> files, it means that the connection to MySQL was not picked up from the config and metabase defaulted to its built-in functionality. DO NOT LEAVE IT AS IS. Investigate, fix, remove the db files, try again. Don't 'let it be, should be fine' - you will regret later.</p>"},{"location":"AWS/Metabase-on-EC2-Amazon-Linux-2022-2023/#testing","title":"Testing","text":"<p>It's time to log in to metabase - using a browser, go to  http://:3000/ you should see the Metabase interface, go through the setup process, add your sources and have fun."},{"location":"AWS/Metabase-on-EC2-Amazon-Linux-2022-2023/#updates","title":"Updates","text":"<p>Stop the service</p> <pre><code>systemctl stop metabase\n</code></pre> <p>Back up the current version</p> <pre><code>cd /home/metabase\nmv metabase.jar metabase_$(date +%d-%m-%Y).jar\n</code></pre> <p>Download the latest version to the same place</p> <pre><code>wget https://downloads.metabase.com/{latest-version}/metabase.jar\n</code></pre> <p>Start the service</p> <pre><code>systemctl start metabase\n</code></pre>"},{"location":"AWS/Metabase-on-EC2-Amazon-Linux-2022-2023/#there-are-some-useful-tips-that-got-me-through-my-stumbles","title":"There are some useful tips that got me through my stumbles.","text":"<p>MySQL developers in their 'wisdom' decided to introduce certain settings that caused a lot of frustration. One of them was inability to do 'group by' in some scenarios.  I know I could rework the metabase 'questions', but life is too short for that.</p> <p>First, check the sql_mode by running this query:</p> <pre><code>SELECT @@sql_mode;\n</code></pre> <p>The result should be something like this:</p> <pre><code>ONLY_FULL_GROUP_BY,STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_ENGINE_SUBSTITUTION\n</code></pre> <p>Make a note of the values, you will be needing most of them. Edit my.cnf under [mysqld] section:</p> <p>sql_mode= (values from above, but remove ONLY_FULL_GROUP_BY)</p> <p>e.g. </p> <pre><code>sql_mode=STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_ENGINE_SUBSTITUTION\n</code></pre> <pre><code>sudo systemctl restart mysqld.service\n</code></pre> <p>(c) Dawid Krysiak https://itisoktoask.me/ </p>"},{"location":"AWS/SSM-agent-update-fails-due-to-AWS-Config/","title":"Symptoms","text":"<p>If you try to update SSM agent on an older instance with AWSConfig installed, SSM Agent will refuse to update, reporting issues caused by AWSConfig</p>"},{"location":"AWS/SSM-agent-update-fails-due-to-AWS-Config/#remediation-steps","title":"Remediation steps","text":""},{"location":"AWS/SSM-agent-update-fails-due-to-AWS-Config/#backup-your-awsconfig-customisation","title":"Backup your AWSConfig customisation","text":"<p>On Windows, config file can be found in C:\\Program Files\\Amazon\\EC2Config\\settings\\config.xml Technically the installer WILL do the backup. But I prefer doing my own backups :)</p>"},{"location":"AWS/SSM-agent-update-fails-due-to-AWS-Config/#download-the-installer","title":"Download the installer","text":"<p>If you are updating AWSConfig from version 3.x to 4.x you are probably ok, but if you are updating from 2.2.12 or earlier, please refer to the AWS documentation as there are issues with backwards compatibility which you have to go around by updating .NET and two stage installation</p> <p>https://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/UsingConfig_Install.html</p> <p>Download and unzip <code>https://s3.amazonaws.com/ec2-downloads-windows/EC2Config/EC2Install.zip</code></p>"},{"location":"AWS/SSM-agent-update-fails-due-to-AWS-Config/#install-awsconfig-through-powershell","title":"Install AWSConfig through PowerShell","text":"<p><code>EC2Install.exe /norestart</code></p>"},{"location":"AWS/SSM-agent-update-fails-due-to-AWS-Config/#revert-back-the-configxml-from-backup","title":"revert back the config.xml from backup","text":""},{"location":"AWS/SSM-agent-update-fails-due-to-AWS-Config/#restart-ec2config-service-through-msconfigmsc","title":"restart EC2Config service through msconfig.msc","text":""},{"location":"AWS/SSM-agent-update-fails-due-to-AWS-Config/#check-the-logs-in-cprogram-filesamazonec2configlogs","title":"Check the logs in C:\\Program Files\\Amazon\\EC2Config\\logs\\","text":"<p>If you see WMI error, that might not be a big problem (I've seen it all over the place), but important part is the report of pushing out reports.</p>"},{"location":"AWS/SSM-agent-update-fails-due-to-AWS-Config/#install-the-latest-ssm-agent","title":"Install the latest SSM agent","text":"<p><code>https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-install-ssm-win.html</code></p> <p>(c) Dawid Krysiak https://itisoktoask.me/</p>"},{"location":"AWS/reduce-the-cost-of-running-AWS-instances/","title":"Lower the cost of AWS EC2","text":""},{"location":"AWS/reduce-the-cost-of-running-AWS-instances/#preface","title":"Preface","text":"<p>Probably most of the people who started their journey with AWS, had their 'ups!' moments when they looked at the Billing dashboard. Learning curve should be steep, but why so expensive? AWS is fogging the view by offering 'free tier' for many of their services in the first year of your account activity (and sometimes permanently!) but understanding of all the 'gotcha!'s escapes even more experienced users and even certified AWS engineers. One of the methods of staying on top of your bill is to set up clear rules in both your billing (e.g. setting up a billing alert if it approaches predefined budget limit) and your learning process. If your account is not set up for production purposes, I bet you don't need EC2 and RDS instances running 24/7? But sometimes it escapes your mind that you should shut down the systems at the end of the day. On top of that, even if you do stop it, RDS will not stay 'stopped' - it will quietly start back up after 7 days, so if you don't have any CloudWatch alarms set up for that instance, you will miss the fact that it's up and running and burning cash. I know, I've been there. Hence this tool.</p>"},{"location":"AWS/reduce-the-cost-of-running-AWS-instances/#purpose","title":"Purpose","text":"<p>aws-instance-stop Deployed as Lambda function, script stops EC2 and RDS instances based on the tags.</p>"},{"location":"AWS/reduce-the-cost-of-running-AWS-instances/#usage","title":"Usage","text":"<ul> <li>Add to your EC2/RDS instance(s) tag 'uptime' with appropriate value (in this example 'daytime').</li> <li>Deploy this script as Lambda</li> <li>If you need the systems to stop/start automatically, create another lambda from the code, just change <code>stop_instances</code> and <code>stop_db_instances</code> to <code>start_instances</code> and <code>start_db_instances</code> respectively </li> <li>Add a trigger(s) 'EventBridge (CloudWatch Events)' (e.g. start lambda triggered at 8AM, stop lambda at 8PM)</li> <li>Create a new rule with appropriate Schedule expression.</li> <li>create new role with policy: </li> </ul> <p><pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"VisualEditor0\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"rds:StopDBInstance\",\n                \"rds:DescribeDBInstances\",\n                \"logs:CreateLogStream\",\n                \"ec2:DescribeInstances\",\n                \"ec2:StopInstances\",\n                \"logs:CreateLogGroup\",\n                \"logs:PutLogEvents\"\n            ],\n            \"Resource\": \"*\"\n        }\n    ]\n}\n</code></pre> Attach it to Lambda as execution policy</p> <p>(c) Dawid Krysiak https://itisoktoask.me/ </p>"},{"location":"AWS/analytics/Amazon-Athena/","title":"Amazon Athena","text":""},{"location":"AWS/analytics/Amazon-Athena/#amazon-athena-is-an-interactive-query-service-that-makes-it-easy-to-analyze-data-directly-in-amazon-simple-storage-service-amazon-s3-using-standard-sql","title":"Amazon Athena is an interactive query service that makes it easy to analyze data directly in Amazon Simple Storage Service (Amazon S3) using standard SQL","text":""},{"location":"AWS/analytics/Amazon-Athena/#buzzwords-when-to-use-athena","title":"Buzzwords / when to use Athena?","text":"<ul> <li>analysis of semi-structured data and structured data stored in Amazon S3 (e.g. CSV, JSON, Apache Parquet, Apache ORC - columnar data formats)</li> <li>ad-hoc queries using ANSI SQL without the need to aggregate or load data into Athena</li> </ul>"},{"location":"AWS/analytics/Amazon-Athena/#integrations","title":"Integrations","text":"<ul> <li>Amazon QuickSight for visualisation</li> <li>Amazon Glue Data Catalog which offers persistent metadata store for your data in Amazon S3, create tables and query data in Athena based on central metadata store and integrated with ETL and data discovery features of AWS Glue.</li> <li>S3 - direct queries over data in S3 without a need to format the data or manage infrastructure (e.g. web logs, WAF logs etc.)</li> </ul>"},{"location":"AWS/analytics/Amazon-EMR/","title":"Amazon EMR","text":""},{"location":"AWS/analytics/Amazon-EMR/#amazon-emr","title":"Amazon EMR","text":""},{"location":"AWS/analytics/Amazon-EMR/#amazon-emr-makes-it-simple-and-cost-effective-to-run-highly-distributed-processing-frameworks-such-as-hadoop-spark-and-presto-when-compared-to-on-premises-deployments-amazon-emr-is-flexible-you-can-run-custom-applications-and-code-and-define-specific-compute-memory-storage-and-application-parameters-to-optimize-your-analytic-requirements","title":"Amazon EMR makes it simple and cost effective to run highly distributed processing frameworks such as Hadoop, Spark, and Presto when compared to on-premises deployments. Amazon EMR is flexible \u2013 you can run custom applications and code, and define specific compute, memory, storage, and application parameters to optimize your analytic requirements.","text":""},{"location":"AWS/analytics/Amazon-EMR/#buzzwords-and-other-big-data-nonsense","title":"Buzzwords and other big-data nonsense","text":"<ul> <li>Hadoop Apache Hadoop is a distributed system to solve problems involving massive ammounts of data and computation. Software framework for distributed storage and processing of big data</li> <li>HDFS a distributed filesystem that stores data on commodity machines, providing very aggregate bandwidth across the cluster</li> <li>Hadoop YARN platform responsible for managing computing resources in clusters and using them for **scheduling users' applications **</li> <li>Hadoop MapReduce an implementation of the MapReduce programming model for large-scale data processing</li> <li>Hadoop Ozone an object store for Hadoop</li> <li>SQL queries</li> <li>Amazon EMR giver you full control over the configuration your cluster and software installed on it (so YARN?)</li> </ul>"},{"location":"AWS/migration/aws-application-discovery-service/","title":"AWS Application Discovery Service","text":"<p>AWS Application Discovery Service helps you plan your migration to the AWS cloud by collecting usage and configuration data about your on-premises servers and databases.</p>","tags":["aws"]},{"location":"AWS/migration/aws-application-discovery-service/#buzzwords","title":"Buzzwords","text":"<ul> <li> <p>Application Discovery Service is integrated with AWS Migration Hub and AWS Database Migration Service Fleet Advisor. </p> </li> <li> <p>Migration Hub aggregates your migration status information into a single console.</p> </li> <li>You can view the discovered servers, group them into applications, and then track the migration status of each application from the Migration Hub console in your home Region. You can use DMS Fleet Advisor to assess migrations options for database workloads.</li> <li> <p>you can export data about the network connections that exist between servers.</p> </li> <li> <p>All discovered data is stored in your AWS Migration Hub home Region. Therefore, you must set your home Region in the Migration Hub</p> </li> <li>data can be exported for analysis in  Amazon Athena and Amazon QuickSight</li> <li> <p>Agentless discovery can be performed by deploying the Application Discovery Service Agentless Collector (Agentless Collector) (OVA file) through your VMware vCenter. After Agentless Collector is configured, it identifies virtual machines (VMs) and hosts associated with vCenter. Agentless Collector collects the following static configuration data: Server hostnames, IP addresses, MAC addresses, disk resource allocations, database engine versions, and database schemas. Additionally, it collects the utilization data for each VM and database providing the average and peak utilization for metrics such as CPU, RAM, and Disk I/O.</p> </li> <li> <p>Agent-based discovery can be performed by deploying the AWS Application Discovery Agent on each of your VMs and physical servers. The agent installer is available for Windows and Linux operating systems. It collects static configuration data, detailed time-series system-performance information, inbound and outbound network connections, and processes that are running.</p> <p>The term host refers to either a physical server or a VM.</p> <p>Collected data is in measurements of kilobytes (KB) unless stated otherwise.</p> <p>Equivalent data in the Migration Hub console is reported in megabytes (MB).</p> <p>The polling period is in intervals of approximately 15 seconds and is sent to AWS every 15 minutes.</p> <p>Data fields denoted with an asterisk (*) are only available in the .csv files that are produced from the agent's API export function.</p> </li> <li> <p>Agentless Collector currently supports data collection from VMware VMs and from database and analytics servers. Future modules will support collection from additional virtualization platforms, and operating system level collection.</p> </li> <li> <p>VMWare Discovery use the Agentless Collector to collect system information without having to install an agent on each VM.Agentless Collector captures system performance information and resource utilization for each VM running in the vCenter, regardless of what operating system is in use. However, it cannot \u201clook inside\u201d each of the VMs, and as such, cannot figure out what processes are running on each VM nor what network connections exist. Therefore, if you need this level of detail and want to take a closer look at some of your existing VMs in order to assist in planning your migration, you can install the Discovery Agent on an as-needed basis.</p> </li> <li> <p>Database Discover The Agentless Collector database and analytics data collection module captures metadata and performance metrics that provide insight into your data infrastructure. The database and analytics data collection module uses LDAP in Microsoft Active Directory to gather information about the OS, database, and analytics servers in your network. Then, the data collection module periodically runs queries to collect actual utilization metrics of CPU, memory, and disk capacity for the databases and analytics servers</p> </li> </ul> <p>To use Application Discovery Service, the following is assumed:</p> <pre><code>You have signed up for AWS. For more information, see Setting up Application Discovery Service.\n\nYou have selected a Migration Hub home Region. For more information, see the documentation regarding home Regions.\n</code></pre> <p>Here's what to expect:</p> <pre><code>The Migration Hub home Region is the only Region where Application Discovery Service stores your discovery and planning data.\n\nDiscovery agents, connectors, and imports can be used in your selected Migration Hub home Region only.\n\nFor a list of AWS Regions where you can use Application Discovery Service, see the Amazon Web Services General Reference.\n</code></pre>","tags":["aws"]},{"location":"Databases/MySQL/","title":"MySQL","text":""},{"location":"Databases/MySQL/#mysql-tunnel","title":"MySQL tunnel","text":"<p>Set up a tunnel through SSH and then use MySQL Workbench on your desktop to connect to 127.0.0.1:3336 rather than directly to the MySQL server.</p> <p><pre><code>ssh -i .\\user.pem -L 3336:127.0.0.1:3306 ec2-user@&lt;IP of the instance&gt;\n</code></pre> Of course, that would also work if the instance was inside of a network.</p> <pre><code> ssh -i .\\user.pem -L 3333:&lt;internal ip&gt;:3306 ec2-user@&lt;gateway / bastion IP&gt;\n</code></pre>"},{"location":"Databases/MySQL/#corrupted-mysql-myisam-tables-only","title":"Corrupted MySQL (MyISAM tables ONLY!!!)","text":"<pre><code>stop mysql(d) process using appropriate command for your OS\n\nmyisamchk -r /var/lib/mysql/*/*.MYI\nstart mysql(d) process using appropriate command for your OS\n</code></pre>"},{"location":"Databases/MySQL/#mysql-load-data-from-csv","title":"MySQL - load data from csv","text":"<pre><code>use db-name;\nLOAD DATA LOCAL INFILE '/report.csv' \nINTO TABLE tickets\nFIELDS TERMINATED BY ',' \nENCLOSED BY ''\nLINES TERMINATED BY '\\n'\nIGNORE 1 ROWS;\n</code></pre>"},{"location":"Databases/MySQL/#aws-rds-mysql-load-data-from-csv","title":"AWS RDS MySQL, load data from csv","text":"<p>Due to managed nature of RDS, 'local infile' will not work. If you use MySQL Workbench, set the system variable (in Advanced connection settings OPT_LOCAL_INFILE=1</p> <pre><code>use reports;\nLOAD DATA INFILE 'report.csv' \nINTO TABLE tickets\nFIELDS TERMINATED BY ',' \nENCLOSED BY ''\nLINES TERMINATED BY '\\n'\nIGNORE 1 ROWS;\n</code></pre>"},{"location":"Features/LaTeX%20Math%20Support/","title":"LaTeX Math Support","text":"<p>LaTeX math is supported using MathJax.</p> <p>Inline math looks like \\(f(x) = x^2\\). The input for this is <code>$f(x) = x^2$</code>. Use <code>$...$</code>.</p> <p>For a block of math, use <code>$$...$$</code> on separate lines</p> <pre><code>$$\nF(x) = \\int^a_b \\frac{1}{2}x^4\n$$\n</code></pre> <p>gives </p> \\[ F(x) = \\int^a_b \\frac{1}{2}x^4 \\]"},{"location":"Features/Mermaid%20Diagrams/","title":"Mermaid diagrams","text":"<p>Here's the example from MkDocs Material documentation: </p> <pre><code>graph LR\n  A[Start] --&gt; B{Error?};\n  B --&gt;|Yes| C[Hmm...];\n  C --&gt; D[Debug];\n  D --&gt; B;\n  B ----&gt;|No| E[Yay!];</code></pre>"},{"location":"Features/Text%20Formatting/","title":"Text Formatting","text":"<p>You can have lists like this</p> <ul> <li>first</li> <li>second</li> <li>third</li> </ul> <p>Or checklist lists to</p> <ul> <li> Get</li> <li> things</li> <li> done</li> </ul> <p>Also, get highlights and strikethroughs as above (similar to Obsidian).</p> <p>More formatting options for your webpage here. (but not compatible with Obsidian)</p>"},{"location":"Networking/","title":"Index","text":"<ul> <li>OSI Model</li> <li>Common ports</li> <li>Port forwards / reverse proxy</li> <li>caching proxy<ul> <li>squid</li> <li>NginX</li> </ul> </li> <li>Load Balancer</li> <li>Firewall</li> <li>NFS</li> <li>SMB</li> <li>Network tools<ul> <li>wget</li> <li>curl</li> <li>scp</li> <li>sftp</li> <li>nmap</li> <li>tcpdump</li> <li>telnet</li> <li>ping</li> <li>traceroute /tracert</li> </ul> </li> </ul>"},{"location":"OperatingSystems/Linux/Apple-Mac-keyboard-on-Linux/","title":"Apple Mac keyboard on Linux","text":""},{"location":"OperatingSystems/Linux/Apple-Mac-keyboard-on-Linux/#preface","title":"Preface","text":"<p>Das Keyboard for Apple systems contain an 'Eject' Key (so the chasis of a computer is nice and slick :)). Problem is, it comes at the cost of 'Insert' key.</p> <p>Solution should work on all Linux distros.</p> <ul> <li>(if doesn't exist) create ~/.Xmodmap <pre><code>touch ~/.Xmodmap\n</code></pre></li> <li>Add a keycode to the end of the file: <pre><code>echo \"keycode 169 = Insert\" &gt;&gt; ~/.Xmodmap\n</code></pre></li> <li>Reload the config <pre><code>xmodmap ~/.Xmodmap\n</code></pre> 'Eject' key should stop ejecting the optical drive tray and act as Insert. Win :)</li> </ul>"},{"location":"OperatingSystems/Linux/Apple-Mac-keyboard-on-Linux/#known-issues","title":"Known issues!!!","text":"<p>For some reason, certain Linux distros (in my case pop!OS) struggle with the above a bit, FREEZING the whole WindowsManager for a few seconds ecery time a terminal window is opened and the above command invoked. So there is another solution.</p> <p><code>echo \"xmodmap -e 'keycode 169 = Insert'\" &gt;&gt; ~/bash_rc</code></p> <p>That will make the screen freeze for a second once - upon login to the desktop, but I see no issues afterwards.</p> <p>(c) Dawid Krysiak https://itisoktoask.me/</p>"},{"location":"OperatingSystems/Linux/FileSystem/","title":"FileSystem","text":"<p>FileSystem-Structure</p> <p>Basic-FileSystem-operations Advanced-FileSystem-operations-partitioning [[Advanced FileSystem operations - LVM]]</p>"},{"location":"OperatingSystems/Linux/Kernel/","title":"Kernel","text":"<p>Kernel functions</p> <p>kernel-compilation</p>"},{"location":"OperatingSystems/Linux/Shell/","title":"Shell","text":"<p>[bash-scripting] [vim] [text-manipulation] [process-monitoring] [network] [source-compilation]</p>","tags":["Linux","Bash"]},{"location":"OperatingSystems/Linux/Stop%20using%20Elastic%20IPs/","title":"Stop using Elastic IPs","text":""},{"location":"OperatingSystems/Linux/Stop%20using%20Elastic%20IPs/#elastic-ip-is-useful-but-gotcha","title":"Elastic IP is useful but... (gotcha!)","text":"<p>If your instance is not up and running 24/7, EIP will incur cost. But what if you wish to attach a domain name without fiddling with ALB... Which without an instance behind it... (you guest it!) will incur cost!. The solution is to make sure that your instance upon every boot updates Route53 record with its new external IP. Let's do it!</p>"},{"location":"OperatingSystems/Linux/Stop%20using%20Elastic%20IPs/#create-iam-role-to-access-route53-from-an-instance","title":"Create IAM role to access Route53 from an instance.","text":"<p>Your instance will have to be able to modify Route53 resources (I added some read permissions useful during learning).</p> <p>Create the below and attach to the IAM role associated with your instance.</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"VisualEditor0\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"route53:GetHostedZone\",\n                \"route53:ListResourceRecordSets\",\n                \"route53:ChangeResourceRecordSets\"\n            ],\n            \"Resource\": \"arn:aws:route53:::hostedzone/&lt;zone id from details&gt;\"\n        },\n        {\n            \"Sid\": \"VisualEditor1\",\n            \"Effect\": \"Allow\",\n            \"Action\": \"route53:ListHostedZones\",\n            \"Resource\": \"*\"\n        }\n    ]\n}\n</code></pre>"},{"location":"OperatingSystems/Linux/Stop%20using%20Elastic%20IPs/#check-if-you-can-read-the-configuration-of-your-r53-resources","title":"Check if you can read the configuration of your R53 resources.","text":"<p><pre><code>aws route53 list-resource-record-sets --hosted-zone-id &lt;zone id from details&gt;\n</code></pre> It should return a JSON response with the zone details.</p>"},{"location":"OperatingSystems/Linux/Stop%20using%20Elastic%20IPs/#check-if-you-can-modify-your-r53-resources","title":"Check if you can modify your R53 resources.","text":"<p>Let's create/update a record. Create a JSON file and fill in the details.</p> <p>( I suggest a looong session with official documentation )</p> <pre><code>{\n  \"Comment\": \"testing\",\n  \"Changes\": [\n    {\n      \"Action\": \"UPSERT\",\n      \"ResourceRecordSet\": {\n        \"Name\": \"host.domain.com\",\n        \"Type\": \"A\",\n        \"TTL\": 60,\n        \"ResourceRecords\": [\n          {\n            \"Value\": \"8.8.8.8\"\n          }\n        ]\n      }\n    }\n  ]\n}\n</code></pre> <p>And let's test it.</p> <pre><code>aws route53 change-resource-record-sets --hosted-zone-id &lt;zone id&gt; --change-batch file://call.json\n</code></pre> <p>If everything goes well, you should see a confirmation:</p> <pre><code>{\n    \"ChangeInfo\": {\n        \"Status\": \"PENDING\",\n        \"Comment\": \"testing\",\n        \"SubmittedAt\": \"2021-12-16T23:07:54.128Z\",\n        \"Id\": \"/change/C04585772S8P7ZILBKLBP\"\n    }\n}\n</code></pre>"},{"location":"OperatingSystems/Linux/Stop%20using%20Elastic%20IPs/#lets-try-to-figure-out-whats-our-external-ip","title":"Let's try to figure out what's our external IP","text":"<p>Some people try to use whatsmyip.com and similar - in case of AWS it's not actually necessary. <code>curl -s http://169.254.169.254/latest/meta-data/public-ipv4</code> should return the public IP allocated to the machine.</p>"},{"location":"OperatingSystems/Linux/Stop%20using%20Elastic%20IPs/#so-let-us-put-it-all-together-shall-we","title":"So let us put it all together, shall we?","text":"<ul> <li>prepare a file <code>/root/template.json</code> and place the JSON payload content used earlier</li> <li>in root's crontab set up a line which will curl the instance metadata system to obtain the IP, then using sed it will replace preexisting 8.8.8.8  from above example with the current IP (note: this means the template.json needs to have one (and one only) IP in the \"ResourceRecords\")</li> </ul> <pre><code>@reboot cd /root ; ip=`curl -s http://169.254.169.254/latest/meta-data/public-ipv4` ;sed -r 's/(\\b[0-9]{1,3}\\.){3}[0-9]{1,3}\\b'/\"$ip\"/ template.json &gt; payload.json ; aws route53 change-resource-record-sets --hosted-zone-id &lt;zone id&gt; --change-batch file://payload.json &gt;&gt; /root/eip_history.log\n</code></pre>"},{"location":"OperatingSystems/Linux/Stop%20using%20Elastic%20IPs/#test-it","title":"Test it.","text":"<p>Stop/start your machine to see if you loose your external (public) IP. If your instance comes up with a new IP, give it a minute or two and check if Route53 record is updated.</p> <p>(c) Dawid Krysiak https://itisoktoask.me/ </p>"},{"location":"OperatingSystems/Linux/how-to-keep-Linux-system-updated/","title":"How to ensure your Linux-based EC2 instance is updated","text":"","tags":["Linux","Bash"]},{"location":"OperatingSystems/Linux/how-to-keep-Linux-system-updated/#without-invoking-external-tools","title":"(without invoking external tools)","text":"<p>AWS users are tempted to grab a new shiny thing for every possible task. But in the majority of cases, built-in tools will suffice.</p> <p>Goal: We need to make sure our system is patched to avoid vulnerabilities. We need to make sure it's patched regardless of the instance uptime - e.g.  machine running 24/7 or onlyu 8h a day.</p>","tags":["Linux","Bash"]},{"location":"OperatingSystems/Linux/how-to-keep-Linux-system-updated/#find-the-right-tool-for-the-job","title":"Find the right tool for the job","text":"<p>There is a number of package managers in Linux world so make sure you pick the right one for your system. </p> <p>How to figure out which system it is? If you are in the console, try:</p> <ul> <li><code>lsb_release -a</code></li> <li> <p>if doesn't work, try:     <code>cat /etc/os-release</code></p> </li> <li> <p>if ubuntu/debian = apt</p> </li> <li>if Amazon/RedHat = yum</li> <li>if newer RedHat / Fedora = dnf</li> </ul>","tags":["Linux","Bash"]},{"location":"OperatingSystems/Linux/how-to-keep-Linux-system-updated/#where-is-the-app","title":"where is the app?","text":"<pre><code>which yum\n\n/usr/bin/yum\n</code></pre>","tags":["Linux","Bash"]},{"location":"OperatingSystems/Linux/how-to-keep-Linux-system-updated/#how-to-schedule-the-update-cron-to-the-rescue","title":"how to schedule the update? CRON to the rescue!","text":"<p>Highly recommend the below website which nicely explains crontab architecture and allows for easy experimentation without consequences.</p> <pre><code>https://crontab.guru/#5_0_*_*_*\n</code></pre>","tags":["Linux","Bash"]},{"location":"OperatingSystems/Linux/how-to-keep-Linux-system-updated/#how-to-make-sure-its-always-triggered","title":"How to make sure it's always triggered?","text":"<p>If we would only implement rigid schedule (e.g. 00:05) if that machine would be stopped overnight, update would never triggered. So w need two entries in the crontab -one to cover 24/7 scenario, one to cover shutdown/reboot scenario.</p> <pre><code># trigger update 5 minutes past midnight every day\n5 0 * * * /usr/bin/yum update -y\n\n# trigger update at system bootup\n@reboot /usr/bin/yum update -y\n</code></pre>","tags":["Linux","Bash"]},{"location":"OperatingSystems/Linux/linux-operating-system/","title":"Linux Operating system","text":"<p>In very basic terms, Linux comprise of 3 'layers'</p> <ol> <li>Kernel</li> <li>FileSystem</li> <li>Shell</li> </ol> <p>Concept of distributions and release cycles.</p> <ul> <li>RedHat family<ul> <li>RHEL</li> <li>Fedora</li> <li>AWS Linux</li> </ul> </li> <li>Debian family<ul> <li>Debian stable</li> <li>Debian unstable</li> <li>Ubuntu</li> </ul> </li> </ul>","tags":["Linux"]},{"location":"OperatingSystems/Linux/linux-useful-one-liners/","title":"Linux useful one liners","text":"","tags":["Linux","Bash"]},{"location":"OperatingSystems/Linux/linux-useful-one-liners/#mysql-tunnel","title":"MySQL tunnel","text":"<p>Set up a tunnel through SSH and then use MySQL Workbench on your desktop to connect to 127.0.0.1:3336 rather than directly to the MySQL server.</p> <p><pre><code>ssh -i .\\user.pem -L 3336:127.0.0.1:3306 ec2-user@&lt;IP of the instance&gt;\n</code></pre> Of course, that would also work if the instance was inside of a network.</p> <pre><code> ssh -i .\\user.pem -L 3333:&lt;internal ip&gt;:3306 ec2-user@&lt;gateway / bastion IP&gt;\n</code></pre>","tags":["Linux","Bash"]},{"location":"OperatingSystems/Linux/linux-useful-one-liners/#system-load-is-high-but-not-caused-by-processes-hammering-cpu-check-io-activity","title":"System load is high, but not caused by processes hammering CPU? Check I/O activity:","text":"<pre><code> iostat -x 1\n</code></pre>","tags":["Linux","Bash"]},{"location":"OperatingSystems/Linux/linux-useful-one-liners/#ntp-time-synchronisation","title":"NTP time synchronisation","text":"<p><pre><code>/usr/sbin/ntpdate -s pool.ntp.org\n</code></pre> if you want to make it scheduled - cronjob solution: <pre><code>0 * * * * /usr/sbin/ntpdate -s pool.ntp.org  &gt;/dev/null 2&gt;&amp;1\n</code></pre></p>","tags":["Linux","Bash"]},{"location":"OperatingSystems/Linux/linux-useful-one-liners/#ntp-time-fails-because-firewall","title":"NTP time fails because firewall?","text":"<pre><code>sudo date -s \"$(wget -S  \"http://www.google.com/\" 2&gt;&amp;1 | grep -E '^[[:space:]]*[dD]ate:' | sed 's/^[[:space:]]*[dD]ate:[[:space:]]*//' | head -1l | awk '{print $1, $3, $2,  $5 ,\"GMT\", $4 }' | sed 's/,//')\"\n</code></pre>","tags":["Linux","Bash"]},{"location":"OperatingSystems/Linux/linux-useful-one-liners/#linux-filesystem-becomes-read-only-the-last-trick-before-reboot","title":"Linux filesystem becomes read-only. The last trick before reboot:","text":"<pre><code> check real status of mounts\n /proc/mounts\n\n check if /etc/fstab entries have errors=remount-ro option on\n  mount -o remount,rw /\n</code></pre>","tags":["Linux","Bash"]},{"location":"OperatingSystems/Linux/linux-useful-one-liners/#killing-the-processes-by-the-name","title":"Killing the processes by the name","text":"<pre><code>ps -ef | grep procesname | grep -v grep | awk '{print $2}'\n</code></pre>","tags":["Linux","Bash"]},{"location":"OperatingSystems/Linux/linux-useful-one-liners/#corrupted-mysql-myisam-tables-only","title":"Corrupted MySQL (MyISAM tables ONLY!!!)","text":"<pre><code>myisamchk -r /var/lib/mysql/*/*.MYI\n</code></pre>","tags":["Linux","Bash"]},{"location":"OperatingSystems/Linux/linux-useful-one-liners/#find-out-the-top-5-memory-consuming-process","title":"Find Out The Top 5 Memory Consuming Process","text":"<pre><code>ps auxf | sort -nr -k 4 | head -5\n</code></pre>","tags":["Linux","Bash"]},{"location":"OperatingSystems/Linux/linux-useful-one-liners/#usage-in-percentage","title":"Usage in percentage:","text":"<pre><code>ps -A --sort -rss -o comm,pmem | head -n 11\n</code></pre>","tags":["Linux","Bash"]},{"location":"OperatingSystems/Linux/linux-useful-one-liners/#find-out-top-10-cpu-consuming-process","title":"Find Out top 10 CPU Consuming Process","text":"<pre><code> ps auxf | sort -nr -k 3 | head -5\n</code></pre>","tags":["Linux","Bash"]},{"location":"OperatingSystems/Linux/linux-useful-one-liners/#devnull-disappeared","title":"/dev/null disappeared?","text":"<pre><code>rm /dev/null; mknod -m 666 /dev/null c 1 3\n</code></pre>","tags":["Linux","Bash"]},{"location":"OperatingSystems/Linux/linux-useful-one-liners/#find-big-logs","title":"Find big logs","text":"<pre><code>find . -name '*.log' -size +400M -exec ls -lha {} \\;\n</code></pre>","tags":["Linux","Bash"]},{"location":"OperatingSystems/Linux/linux-useful-one-liners/#inodes-where-did-they-went","title":"Inodes where did they went?","text":"<p><pre><code>find . -xdev -printf '%h\\n' | sort | uniq -c | sort -k 1 -n\n</code></pre> Another aproach: <pre><code> for i in `find . -xdev -type d `; do echo `ls -a $i | wc -l` $i; done | sort -n\n</code></pre> or another approach: <pre><code>du --inodes -S | sort -rh | sed -n '1,50{/^.\\{71\\}/s/^\\(.\\{30\\}\\).*\\(.\\{37\\}\\)$/\\1...\\2/;p}'\n</code></pre></p>","tags":["Linux","Bash"]},{"location":"OperatingSystems/Linux/linux-useful-one-liners/#tired-of-known-hots-error-while-using-scp","title":"Tired of known hots error while using scp?","text":"<pre><code>alias scp_nocheck='scp -o '\\''StrictHostKeyChecking no'\\'' -o '\\''CheckHostIP no'\\'' -o '\\''HashKnownHosts no'\\'' -o '\\''UserKnownHostsFile /dev/null'\\'''\n</code></pre>","tags":["Linux","Bash"]},{"location":"OperatingSystems/Linux/linux-useful-one-liners/#tired-of-known-hosts-error","title":"Tired of known hosts error?","text":"<pre><code>alias ssh_nocheck='ssh -o '\\''StrictHostKeyChecking no'\\'' -o '\\''CheckHostIP no'\\'' -o '\\''HashKnownHosts no'\\'' -o '\\''UserKnownHostsFile /dev/null'\\'''\n</code></pre>","tags":["Linux","Bash"]},{"location":"OperatingSystems/Linux/linux-useful-one-liners/#console-version-of-the-above-if-you-have-direct-access-to-logs","title":"Console version of the above if you have direct access to logs","text":"<pre><code>cat logfile | grep -E -o \"([0-9]{1,3}[.]){3}[0-9]{1,3}\" | sort | uniq -c | sort -rn\n</code></pre>","tags":["Linux","Bash"]},{"location":"OperatingSystems/Linux/linux-useful-one-liners/#cpu-load-is-low-but-machine-feels-unresponsive","title":"CPU load is low, but machine feels unresponsive?","text":"<pre><code>iostat -d -x 5 3\n\n        rrqm/s : The number of read requests merged per second that were queued to the hard disk\n        wrqm/s : The number of write requests merged per second that were queued to the hard disk\n        r/s : The number of read requests per second\n        w/s : The number of write requests per second\n        rsec/s : The number of sectors read from the hard disk per second\n        wsec/s : The number of sectors written to the hard disk per second\n        avgrq-sz : The average size (in sectors) of the requests that were issued to the device.\n        avgqu-sz : The average queue length of the requests that were issued to the device\n        await : The average time (in milliseconds) for I/O requests issued to the device to be served. This includes the time spent by the requests in queue and the time spent servicing them.\n        svctm : The average service time (in milliseconds) for I/O requests that were issued to the device\n        %util : Percentage of CPU time during which I/O requests were issued to the device (bandwidth utilization for the device). Device saturation occurs when this value is close to 100%.\n</code></pre>","tags":["Linux","Bash"]},{"location":"OperatingSystems/Linux/linux-useful-one-liners/#which-process-opened-a-tcpip-port","title":"Which process opened a TCP/IP port?","text":"<pre><code>netstat -tulpn\n</code></pre>","tags":["Linux","Bash"]},{"location":"OperatingSystems/Linux/linux-useful-one-liners/#last-15-modified-files","title":"Last 15 modified files","text":"<pre><code>last 15 modified files\nfind . -type f -printf '%T@ %TY-%Tm-%Td %TH:%TM:%.2TS %P\\n' | sort -nr | head -n 15 | cut -f2- -d\" \"\n</code></pre>","tags":["Linux","Bash"]},{"location":"OperatingSystems/Linux/linux-useful-one-liners/#check-file-type-of-the-files","title":"Check file type of the files","text":"<pre><code>find . -type f | xargs file\n</code></pre>","tags":["Linux","Bash"]},{"location":"OperatingSystems/Linux/linux-useful-one-liners/#find-duplicate-files-using-md5","title":"Find duplicate files using MD5","text":"<pre><code>find . -type f - print0 | xargs -0 md5sum | sort | uniq -w32 --all-repeated=separate\n</code></pre>","tags":["Linux","Bash"]},{"location":"OperatingSystems/Linux/linux-useful-one-liners/#kill-defunct-processes","title":"Kill defunct processes","text":"<p>Not recommended (defunct processes should be killed automatically when the parent process finished) but sometimes 'you gotta do, what you gotta do' <pre><code>ps -ef | grep -E \"\\[ process name \\] &lt;defunct&gt;\" | awk '{print \"kill -9\", $3}' | ksh &gt; /dev/null 2&gt;&amp;1\n</code></pre></p>","tags":["Linux","Bash"]},{"location":"OperatingSystems/Linux/linux-useful-one-liners/#command-line-json-linting","title":"Command line json linting","text":"<pre><code>json_xs -t none &lt; file.json\n</code></pre>","tags":["Linux","Bash"]},{"location":"OperatingSystems/Linux/linux-useful-one-liners/#linux-swap-file-i-cant-even-count-how-many-times-i-used-it-in-the-last-few-weeks-pasting-it-here","title":"Linux swap file (I can't even count how many times I used it in the last few weeks, pasting it here).","text":"<pre><code>sudo fallocate -l 1G /swapfile\nsudo chmod 600 /swapfile\nsudo mkswap /swapfile &amp;&amp; sudo swapon /swapfile\n</code></pre>","tags":["Linux","Bash"]},{"location":"OperatingSystems/Linux/linux-useful-one-liners/#sed-string-replacement-in-multiple-files-at-once","title":"Sed string replacement in multiple files at once.","text":"<p>Quick invention inspired by the need to tidy up someone else's code. This particular oneliner looks for files in the currect directory with .py etension, executes a replacemend of a comment tag with a lighter one, and applies to all files matching the pattern.</p> <pre><code>find . -type f -name \"*.py\" -exec sed -i \"s/\\\"\\\"\\\"/'''/g\" {} +\n</code></pre> <p>(c) Dawid Krysiak https://itisoktoask.me/ </p>","tags":["Linux","Bash"]},{"location":"OperatingSystems/MacOS/CloudWatch%20agent%20on%20MacOS/","title":"CloudWatch agent on MacOS","text":"","tags":["aws","macos","ec2"]},{"location":"OperatingSystems/MacOS/CloudWatch%20agent%20on%20MacOS/#download-the-installation-package","title":"Download the installation package","text":"<p>https://s3.amazonaws.com/amazoncloudwatch-agent/darwin/amd64/latest/amazon-cloudwatch-agent.pkg</p>","tags":["aws","macos","ec2"]},{"location":"OperatingSystems/MacOS/CloudWatch%20agent%20on%20MacOS/#allow-it-to-install","title":"Allow it to install","text":"<p>When you try to install it, you will be met with security restriction messages. Go to System preferences -&gt; Security and Privacy -&gt; unlock the padlock (might ask for password, should be the same as the main user) -&gt; allow apps from store and identified developers -&gt;ok</p>","tags":["aws","macos","ec2"]},{"location":"OperatingSystems/MacOS/CloudWatch%20agent%20on%20MacOS/#configure-the-agent","title":"Configure the agent","text":"<p>In command line:</p> <pre><code>sudo /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-config-wizard\n</code></pre> <p>Answer the questions as required. The config file is also located at /opt/aws/amazon-cloudwatch-agent/bin/config.json.</p>","tags":["aws","macos","ec2"]},{"location":"OperatingSystems/MacOS/CloudWatch%20agent%20on%20MacOS/#start-the-agent-using-the-configjson-generated-by-the-above","title":"start the agent using the config.json generated by the above.","text":"<pre><code>sudo /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl -a fetch-config -m ec2 -c file:/opt/aws/amazon-cloudwatch-agent/bin/config.json -s\n</code></pre>","tags":["aws","macos","ec2"]},{"location":"OperatingSystems/MacOS/CloudWatch%20agent%20on%20MacOS/#check-cloudwatch-metrics-for-the-new-data","title":"check CloudWatch metrics for the new data.","text":"<p>(c) Dawid Krysiak https://itisoktoask.me/ </p>","tags":["aws","macos","ec2"]},{"location":"OperatingSystems/Windows/Export%20command%20in%20Windows/","title":"How to implement export command from Linux in Windows","text":""},{"location":"OperatingSystems/Windows/Export%20command%20in%20Windows/#check-if-you-have-a-powershell-profile-set-up","title":"check if you have a PowerShell profile set up.","text":"<p>In PowerShell (not the classic CLI!) <pre><code>Test-Path $PROFILE\n</code></pre> The expected result is <code>False</code>, if it returns <code>True</code> you already have a profile set up!</p>"},{"location":"OperatingSystems/Windows/Export%20command%20in%20Windows/#create-a-profile-force-recreates-the-profile-if-one-already-exists","title":"Create a profile (-Force recreates the profile if one already exists)","text":"<pre><code>New-Item \u2013Path $Profile \u2013Type File \u2013Force\n</code></pre>"},{"location":"OperatingSystems/Windows/Export%20command%20in%20Windows/#create-a-module","title":"Create a module","text":"<p>The above command should have printed out the path to your PowerShell profile NOTE: for some reason, different versions of powershell might create a different folder name. I know of WindowsPowerShell and Powershell, so be vigilant and adjust the following commands accordingly. <pre><code>Directory: C:\\Users\\&lt;user&gt;\\Documents\\WindowsPowerShell\n\nMicrosoft.PowerShell_profile.ps1\n</code></pre></p>"},{"location":"OperatingSystems/Windows/Export%20command%20in%20Windows/#create-modules-folder","title":"create modules folder","text":"<pre><code>cd C:\\Users\\&lt;user&gt;\\Documents\\Powershell\nmkdir modules\ncd modules\nmkdir export\ncd export\nnotepad export.psm1\n</code></pre> <p>paste the below code to the file and save</p> <pre><code>Function export{\n    Param ($linuxExport)\n        try {\n            $CharArray =$linuxExport.Split(\"=\")\n            [Environment]::SetEnvironmentVariable( $CharArray[0], $CharArray[1])\n            $var = $CharArray[0]\n            \"Environment Variable Set - $var\"   | write-host -fore green; \n          }\n          catch {\n          \"Expected: export variable=value\"  | write-host -fore red; \n         }\n    }\n</code></pre>"},{"location":"OperatingSystems/Windows/Export%20command%20in%20Windows/#test","title":"test","text":"<p>Exit the current PowerShell session and open another one. execute export command</p> <p><pre><code>export\n</code></pre> the result should be</p> <pre><code>Expected: export variable=value\n</code></pre>"},{"location":"OperatingSystems/Windows/Windows%20useful%20one-liners/","title":"Windows related useful oneliners","text":""},{"location":"OperatingSystems/Windows/Windows%20useful%20one-liners/#windows-reboot-if-there-is-no-access-to-the-graphical-interface","title":"Windows reboot if there is no access to the graphical interface","text":"<pre><code>c:\\windows\\system32\\shutdown -t 0 -r -f\n</code></pre>"},{"location":"OperatingSystems/Windows/Windows%20useful%20one-liners/#windows-equivalent-of-diff","title":"Windows equivalent of diff","text":"<pre><code>FC /B file1 file 2\n</code></pre>"},{"location":"OperatingSystems/Windows/Windows%20useful%20one-liners/#windows-directory-content-comparison","title":"Windows directory content comparison","text":"<pre><code>comp dir1 dir2\n</code></pre>"},{"location":"OperatingSystems/Windows/Windows%20useful%20one-liners/#problems-with-vpn-software-in-windows","title":"Problems with VPN software in Windows?","text":"<p>VPN applications usually create a virtual network interface (normally not visible in the interface but can be un-hidden).  Problem is - there is (was?) a limit to how many network interfaces (net filters) you can have in Windows. If apps refuse to install/start with network interfaces related errors, try to increase the number of net filters &gt; 10 <pre><code>HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Control\\Network\\MaxNumFilters****\n</code></pre></p>"},{"location":"OperatingSystems/Windows/Windows%20useful%20one-liners/#detect-windows-version-through-powershell-useful-if-you-only-have-access-to-ssm-no-credentals-to-the-rdp","title":"Detect Windows version through PowerShell (useful if you only have access to SSM, no credentals to the RDP)","text":"<pre><code>(Get-WmiObject Win32_OperatingSystem).Caption\n</code></pre> <p>(c) Dawid Krysiak https://itisoktoask.me/ </p>"},{"location":"Terminal/vim-magic/","title":"Vim magic","text":""},{"location":"Terminal/vim-magic/#why-its-here-there-are-so-many-tutorials-online","title":"Why it's here, there are so many tutorials online?","text":"<p>I decided to give it a go and ditch visual IDEs completely. Trying to make neovim my primary text editor / IDE. Keeping the notes here, just in case I need a cheat sheet. Basics of vi openvim</p>"},{"location":"Terminal/vim-magic/#copy-paste-within-the-same-file","title":"copy / paste within the same file","text":"<ul> <li>yy to yank one line, p to paste</li> <li>ctrl+v to engage visual select, y to yank (or d to delete/cut), p to paste</li> </ul>"},{"location":"Terminal/vim-magic/#vimdiff-copy-between-leftright-panes","title":"vimdiff copy between left/right panes","text":"<ul> <li>Shift+V to select a line</li> <li>k or j; { or }; up or down keys to select more lines</li> <li>y (yank/copy selected lines)</li> <li>ctrl+w, left/right to move to the other pane</li> <li>p (paste)</li> </ul>"},{"location":"Terminal/vim-magic/#format-json","title":"format JSON","text":"<ul> <li>:%!python -m json.tool</li> </ul> <p>(c) Dawid Krysiak https://itisoktoask.me/</p>"},{"location":"Terminal/weird-issues-with-grep-over-logs/","title":"Weird issue I've never seen before","text":"","tags":["AWS","EC2","Linux","windows","Bash","terminal"]},{"location":"Terminal/weird-issues-with-grep-over-logs/#symptoms","title":"Symptoms","text":"<pre><code>cat /var/log/logfile.log | grep \"magic\"\nBinary file (standard input) matches\n</code></pre>","tags":["AWS","EC2","Linux","windows","Bash","terminal"]},{"location":"Terminal/weird-issues-with-grep-over-logs/#so-lets-check-the-file-datatype","title":"So let's check the file datatype:","text":"<pre><code>file /var/log/logifle.log\ndata\n</code></pre>","tags":["AWS","EC2","Linux","windows","Bash","terminal"]},{"location":"Terminal/weird-issues-with-grep-over-logs/#what-is-happening","title":"What is happening?","text":"<p>So, somehow, a text based log file is identified as binary? It's apparently caused by a NULL character spilled to the log</p>","tags":["AWS","EC2","Linux","windows","Bash","terminal"]},{"location":"Terminal/weird-issues-with-grep-over-logs/#solution","title":"Solution","text":"<p>Solution is to filter out the NULL character first.</p> <pre><code>cat /var/log/logfile.log |tr -d '\\000'| grep \"magic\"\nmagic\n</code></pre>","tags":["AWS","EC2","Linux","windows","Bash","terminal"]},{"location":"Topic%201/Note%201/","title":"Note 1","text":"<p>Example: link to Mermaid Diagrams under <code>Features</code></p>"},{"location":"Topic%201/Note%202/","title":"Note 2","text":""},{"location":"blog/","title":"Blog","text":""}]}